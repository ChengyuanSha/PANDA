{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "2. Node Classification.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "F1op-CbyLuN4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d8fae9ee-6f6d-4e6e-9e73-2e4d98f33292"
   },
   "source": [
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch\n"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Graph Neural Networks (GNNs) to the task of node classification**.\n",
    "Here, we are given the ground-truth labels of only a small subset of nodes, and want to infer the labels for all the remaining nodes (*transductive learning*).\n",
    "\n",
    "To demonstrate, we make use of the `Cora` dataset, which is a **citation network** where nodes represent documents.\n",
    "Each node is described by a 1433-dimensional bag-of-words feature vector.\n",
    "Two documents are connected if there exists a citation link between them.\n",
    "The task is to infer the category of each document (7 in total).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "imGrKO5YH11-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "373bf1d0-9ad2-469a-caf5-ebed4f4743a3"
   },
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n",
      "\n",
      "Dataset: Cora():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])\n",
      "===========================================================================================================\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "torch.set_printoptions(edgeitems=3000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "2708"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(140)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(data.train_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "140"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_mask.sum().item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "500"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.val_mask.sum().item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.test_mask.sum().item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(7781)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fmXWs1dKIzD8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "outputId": "3646967f-a64c-4705-b9a6-2824ce2b7dc7"
   },
   "source": [
    "from src.GCN import GCNStack\n",
    "model = GCNStack(data.num_node_features, hidden_dim1=128, hidden_dim2=16, output_dim=dataset.num_classes)\n",
    "print(model)"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNStack(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(1433, 128)\n",
      "    (1): GCNConv(128, 16)\n",
      "    (2): GCNConv(16, 16)\n",
      "  )\n",
      "  (lns): ModuleList(\n",
      "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (post_mp): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.9241. Test accuracy: 0.2160\n",
      "Epoch 10. Loss: 1.6689. Test accuracy: 0.2710\n",
      "Epoch 20. Loss: 1.2258. Test accuracy: 0.3770\n",
      "Epoch 30. Loss: 0.9460. Test accuracy: 0.6220\n",
      "Epoch 40. Loss: 0.5951. Test accuracy: 0.6830\n",
      "Epoch 50. Loss: 0.4025. Test accuracy: 0.7730\n",
      "Epoch 60. Loss: 0.1766. Test accuracy: 0.7570\n",
      "Epoch 70. Loss: 0.3255. Test accuracy: 0.6910\n",
      "Epoch 80. Loss: 0.0685. Test accuracy: 0.7540\n",
      "Epoch 90. Loss: 0.0849. Test accuracy: 0.7850\n",
      "Epoch 100. Loss: 0.1008. Test accuracy: 0.7830\n",
      "Epoch 110. Loss: 0.0472. Test accuracy: 0.7730\n",
      "Epoch 120. Loss: 0.0256. Test accuracy: 0.7800\n",
      "Epoch 130. Loss: 0.0891. Test accuracy: 0.7600\n",
      "Epoch 140. Loss: 0.0616. Test accuracy: 0.7790\n",
      "Epoch 150. Loss: 0.0815. Test accuracy: 0.7650\n",
      "Epoch 160. Loss: 0.0547. Test accuracy: 0.7460\n",
      "Epoch 170. Loss: 0.0180. Test accuracy: 0.7580\n",
      "Epoch 180. Loss: 0.0690. Test accuracy: 0.7620\n",
      "Epoch 190. Loss: 0.1608. Test accuracy: 0.7350\n",
      "Epoch 200. Loss: 0.0620. Test accuracy: 0.7700\n"
     ]
    }
   ],
   "source": [
    "def model_test(loader, model, is_validation=False, is_training=False):\n",
    "    ''' Testing Code of the Model '''\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            emb, pred = model(data.x, data.edge_index)\n",
    "            pred = pred.argmax(dim=1)\n",
    "            label = data.y\n",
    "\n",
    "        if is_training:\n",
    "            mask = data.val_mask if is_validation else data.train_mask\n",
    "        else: # testing\n",
    "            mask = data.val_mask if is_validation else data.test_mask\n",
    "        # node classification: only evaluate on nodes in test set\n",
    "        pred = pred[mask]\n",
    "        label = data.y[mask]\n",
    "\n",
    "        correct += pred.eq(label).sum().item()\n",
    "    total = 0\n",
    "    for data in loader.dataset:\n",
    "        if is_training:\n",
    "            total += torch.sum(data.train_mask).item()\n",
    "        else:\n",
    "            total += torch.sum(data.test_mask).item()\n",
    "    return correct / total\n",
    "\n",
    "def model_train(dataset, writer, model, epoch_num, lr, weight_decay):\n",
    "    ''' Training code of the model '''\n",
    "    test_loader = loader = DataLoader(dataset, shuffle=False)\n",
    "\n",
    "    # Optimizer\n",
    "    # opt = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # visualize the model architecture in tensorboard\n",
    "    # writer.add_graph(model, ( data.x, data.edge_index ))\n",
    "\n",
    "    # Training:\n",
    "    for epoch in range(epoch_num + 1):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            #print(batch.train_mask, '----')\n",
    "            opt.zero_grad()\n",
    "            embedding, pred = model(batch.x, batch.edge_index)\n",
    "            label = batch.y\n",
    "            pred = pred[batch.train_mask]\n",
    "            label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_acc = model_test(test_loader, model, is_training=False)\n",
    "            print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
    "                epoch, total_loss, test_acc))\n",
    "            writer.add_scalar(\"test accuracy\", test_acc, epoch)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            name = 'epoch' + str(epoch)\n",
    "            writer.add_embedding(embedding, global_step=epoch, tag=name, metadata=batch.y)\n",
    "\n",
    "    return model\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "model = model_train(dataset, writer, model, epoch_num=200, lr=0.01, weight_decay=4e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}